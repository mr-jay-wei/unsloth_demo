# merge_model.py
#
# ç›®çš„: å°†Unsloth LoRAé€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹åˆå¹¶ï¼Œ
#       å¹¶è¿›è¡Œæ¸…ç†å’Œé…ç½®ï¼Œç”Ÿæˆä¸€ä¸ªå¯ä»¥ç›´æ¥è¢«vLLMéƒ¨ç½²çš„ã€
#       å…¼å®¹Chat APIçš„æœ€ç»ˆæ¨¡å‹æ–‡ä»¶å¤¹ã€‚
#
# è¿è¡Œæ–¹å¼: python merge_model.py

import json
import os
import shutil
from unsloth import FastLanguageModel

# ==========================================================
# 1. é…ç½®å‚æ•° (ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´)
# ==========================================================
max_seq_length = 2048
dtype = None
load_in_4bit = True

# è¿™æ˜¯ä½ è®­ç»ƒå¥½çš„LoRAé€‚é…å™¨æ‰€åœ¨çš„æ–‡ä»¶å¤¹
lora_model_dir = "lora_model"

# è¿™æ˜¯æˆ‘ä»¬æœ€ç»ˆè¦ç”Ÿæˆçš„ã€ç”¨äºéƒ¨ç½²çš„å®Œæ•´æ¨¡å‹æ–‡ä»¶å¤¹
output_dir = "merged_model"


# ==========================================================
# 2. åŠ è½½LoRAæ¨¡å‹
# ==========================================================
print(f"Loading LoRA model from: {lora_model_dir}")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=lora_model_dir,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)
print("LoRA model loaded successfully.")

# ==========================================================
# 3. å®šä¹‰å¹¶åº”ç”¨è‡ªå®šä¹‰çš„AlpacaèŠå¤©æ¨¡æ¿
# ==========================================================
# è¿™ä¸ªJinja2æ¨¡æ¿èƒ½å¤Ÿç²¾ç¡®åœ°å°†OpenAIæ ¼å¼çš„`messages`åˆ—è¡¨ï¼Œ
# è½¬æ¢æˆæˆ‘ä»¬å¾®è°ƒæ—¶ä½¿ç”¨çš„Alpacaæ ¼å¼çš„å•ä¸ªå­—ç¬¦ä¸²ã€‚
alpaca_chat_template = (
    "{% for message in messages %}"
        "{% if message['role'] == 'user' %}"
            "{{ 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n' }}"
            "{{ '### Instruction:\\n' + message['content'] + '\\n\\n' }}"
            "{{ '### Input:\\n' + '\\n\\n' }}"  # å‡è®¾inputä¸ºç©ºï¼Œå¦‚æœä½ çš„åº”ç”¨éœ€è¦å¤„ç†inputï¼Œè¿™é‡Œéœ€è¦ä¿®æ”¹
        "{% elif message['role'] == 'assistant' %}"
            "{{ '### Response:\\n' + message['content'] + eos_token + '\\n\\n' }}"
        "{% endif %}"
    "{% endfor %}"
    "{% if add_generation_prompt %}"
        "{{ '### Response:\\n' }}"
    "{% endif %}"
)

print("Applying custom Alpaca chat template...")
tokenizer.chat_template = alpaca_chat_template
print("Chat template applied.")


# ==========================================================
# 4. åˆå¹¶LoRAæƒé‡
# ==========================================================
print(f"Merging LoRA weights and saving to: {output_dir}")
# ä½¿ç”¨ `save_pretrained_merged` å°†æ¨¡å‹å’Œtokenizerä¿å­˜åˆ°è¾“å‡ºç›®å½•
# è¿™ä¼šç”Ÿæˆä¸€ä¸ª16ä½çš„é«˜ç²¾åº¦æ¨¡å‹
model.save_pretrained_merged(output_dir, tokenizer, save_method="hf")
print("Model and tokenizer merged and saved.")


# ==========================================================
# 5. ä¿®å¤é…ç½®æ–‡ä»¶ä»¥å…¼å®¹vLLM
# ==========================================================
# è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼Œå®ƒè§£å†³äº†æˆ‘ä»¬ä¹‹å‰é‡åˆ°çš„ä¸¤ä¸ªéƒ¨ç½²é”™è¯¯

# 5.1 å¤åˆ¶åŸºç¡€æ¨¡å‹çš„config.jsonï¼Œå› ä¸ºå®ƒåŒ…å«äº†æ­£ç¡®çš„æ¨¡å‹æ¶æ„ä¿¡æ¯
base_model_config_path = model.config.name_or_path
if os.path.isdir(base_model_config_path):
    # é€šå¸¸UnslothåŠ è½½åï¼Œè¿™ä¸ªè·¯å¾„å°±æ˜¯ç¼“å­˜ä¸­çš„è·¯å¾„
    config_file_path = os.path.join(base_model_config_path, "config.json")
    if os.path.exists(config_file_path):
        print(f"Copying config.json from base model at: {base_model_config_path}")
        shutil.copy(config_file_path, output_dir)

# 5.2 è¯»å–åˆšåˆšå¤åˆ¶æˆ–ä¿å­˜çš„config.jsonï¼Œå¹¶ç§»é™¤ä¸å…¼å®¹çš„quantization_config
config_path = os.path.join(output_dir, "config.json")
if os.path.exists(config_path):
    print(f"Cleaning config.json at: {config_path}")
    with open(config_path, "r") as f:
        config_data = json.load(f)

    if "quantization_config" in config_data:
        del config_data["quantization_config"]
        print("Removed 'quantization_config' section.")

    with open(config_path, "w") as f:
        json.dump(config_data, f, indent=2)
    print("Config file cleaned and saved.")
else:
    print(f"WARNING: config.json not found at {config_path}. vLLM deployment might fail.")

print("\nğŸš€ All steps completed successfully!")
print(f"Your final model for vLLM deployment is ready at: {output_dir}")