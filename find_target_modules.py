from transformers import AutoModelForCausalLM
import torch
import re

MODEL_NAME = "unsloth/llama-3-8b-bnb-4bit"   # æ”¹æˆä½ çš„æ¨¡å‹åç§°

print(f"Loading model: {MODEL_NAME}")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
)

# === ç¬¬ä¸€æ­¥ï¼šåˆ—å‡ºæ‰€æœ‰æ¨¡å—åç§° ===
module_names = [name for name, _ in model.named_modules()]

print(f"\nå…±å‘ç° {len(module_names)} ä¸ªæ¨¡å—")

# === ç¬¬äºŒæ­¥ï¼šè‡ªåŠ¨è¯†åˆ«æ¨¡å¼ ===
# æ€è·¯ï¼šå¦‚æœå¤šä¸ªå±‚çš„å‘½åæ¨¡å¼é‡å¤å‡ºç°ï¼Œè¯´æ˜é‚£æ˜¯â€œå±‚ç»“æ„â€ä¸­çš„é€šç”¨ç»„ä»¶
# æ¯”å¦‚ model.layers.0.self_attn.q_proj, model.layers.1.self_attn.q_proj ...
# æˆ‘ä»¬æ‰¾å‡ºè¿™äº›â€œæœ«å°¾å­—æ®µåœ¨ä¸åŒå±‚éƒ½å‡ºç°è¿‡â€çš„æ¨¡å—
suffix_count = {}
for name in module_names:
    suffix = name.split(".")[-1]
    suffix_count[suffix] = suffix_count.get(suffix, 0) + 1

# === ç¬¬ä¸‰æ­¥ï¼šç­›é€‰å‡ºé‡å¤å‡ºç°çš„å­æ¨¡å—åï¼ˆå¯èƒ½æ˜¯æ¯å±‚éƒ½æœ‰çš„ç»„ä»¶ï¼‰===
repeated_suffixes = [k for k, v in suffix_count.items() if v > 5]  # å‡ºç°5æ¬¡ä»¥ä¸Š
print("\nğŸ“Š åœ¨å¤šä¸ªå±‚ä¸­é‡å¤å‡ºç°çš„æ¨¡å—ï¼š")
print(repeated_suffixes)

# === ç¬¬å››æ­¥ï¼šåœ¨è¿™äº›å€™é€‰å±‚é‡Œè¿›ä¸€æ­¥ç­›é€‰å‡ºå¸¸è§çš„çº¿æ€§å±‚ ===
candidate_modules = []
for name, module in model.named_modules():
    if hasattr(module, "weight"):  # æœ‰æƒé‡
        if any(name.endswith(suffix) for suffix in repeated_suffixes):
            if isinstance(module, torch.nn.Linear):  # ä¸€èˆ¬ LoRA åªä½œç”¨åœ¨çº¿æ€§å±‚ä¸Š
                candidate_modules.append(name)

print("\nğŸ¯ è‡ªåŠ¨æ£€æµ‹åˆ°çš„å¯å¾®è°ƒæ¨¡å—å€™é€‰ï¼š")
for name in candidate_modules[:50]:  # æ‰“å°å‰50ä¸ª
    print(name)

# === ç¬¬äº”æ­¥ï¼šç”Ÿæˆ target_modules åˆ—è¡¨ ===
unique_targets = sorted(set(
    name.split(".")[-1] for name in candidate_modules
))
print("\nâœ… æ¨èçš„ target_modules =")
print(unique_targets)
